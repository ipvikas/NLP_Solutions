{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##1. Get authenticated from tweeter API"
      ],
      "metadata": {
        "id": "3gYxhTzPXLXK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWpimnTypdvo"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd '/content/drive/My Drive/Colab Notebooks/NLP'\n",
        "\n",
        "apikey= 'du9vEcqQFCtdu9vEcqQFCt'\n",
        "apisecretkey= 'w7JsH0E4KCfOTMdcEw7JsH0E4KCfOTMdcE'\n",
        "accesstoken='126864770-A5bLHXKcDFFk126864770-A5bLHXKcDFFk'\n",
        "accesstokensecret='JS1l5hXn9dSM8lfJS1l5hXn9dSM8lf'\n",
        "\n",
        "import os\n",
        "import tweepy as tw\n",
        "\n",
        "auth = tw.OAuthHandler(apikey, apisecretkey)\n",
        "auth.set_access_token(accesstoken, accesstokensecret)\n",
        "\n",
        "api = tw.API(auth,wait_on_rate_limit=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir(api)"
      ],
      "metadata": {
        "id": "JrzOiPhtaMG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2: Know what's on my timeline"
      ],
      "metadata": {
        "id": "xj36wdk6bC85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_timeline = api.home_timeline()\n",
        "for status in my_timeline:\n",
        "  #print(status.text)\n",
        "  print(vars(status))"
      ],
      "metadata": {
        "id": "E3FdbeCzbA7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = set()\n",
        "allowed_types = [str,int]\n",
        "tweets_data = []\n",
        "for status in my_timeline:\n",
        "  status_dict = dict(vars(status))\n",
        "  keys = vars(status).keys()\n",
        "  single_tweet_data = {'user':status.user.screen_name, 'author':status.author.screen_name}\n",
        "  for k in keys:\n",
        "    try:\n",
        "      #print(k)\n",
        "      v_type=type(status_dict[k])\n",
        "    except:\n",
        "      v_type = None\n",
        "    if v_type != None:\n",
        "\n",
        "        if v_type in allowed_types:\n",
        "          single_tweet_data[k] = status_dict[k]\n",
        "          columns.add(k)\n",
        "\n",
        "  tweets_data.append(single_tweet_data)\n",
        "header_cols = list(columns)\n",
        "\n",
        "header_cols.append('user')\n",
        "header_cols.append('author')\n",
        "header_cols"
      ],
      "metadata": {
        "id": "cMh2JvpmbA-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_data"
      ],
      "metadata": {
        "id": "hbN6M79zbBBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(tweets_data,columns = header_cols)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "nRAoPWe6eLdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3: What's on different user's timeline"
      ],
      "metadata": {
        "id": "Oq6-99pklyhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_timeline_as_df(timeline_list):\n",
        "    columns = set()\n",
        "    allowed_types = [str, int]\n",
        "    tweets_data = []\n",
        "    for status in timeline_list:\n",
        "        status_dict = dict(vars(status))\n",
        "        keys = status_dict.keys()\n",
        "        single_tweet_data = {\"user\": status.user.screen_name, \"author\": status.author.screen_name}\n",
        "        for k in keys:\n",
        "            try:\n",
        "                v_type = type(status_dict[k])\n",
        "            except:\n",
        "                v_type = None\n",
        "            if v_type != None:\n",
        "                if v_type in allowed_types:\n",
        "                    single_tweet_data[k] = status_dict[k]\n",
        "                    columns.add(k)\n",
        "        tweets_data.append(single_tweet_data)\n",
        "\n",
        "\n",
        "    header_cols = list(columns)\n",
        "    header_cols.append(\"user\")\n",
        "    header_cols.append('author')\n",
        "    df = pd.DataFrame(tweets_data, columns=header_cols)\n",
        "    return df"
      ],
      "metadata": {
        "id": "U2iB22V0eLgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_timeline = api.home_timeline()"
      ],
      "metadata": {
        "id": "LATWpI99eLja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = extract_timeline_as_df(my_timeline)\n",
        "df2.head()"
      ],
      "metadata": {
        "id": "OoQCPUF0eLmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user = api.get_user(\"sudhirchaudhary\")\n",
        "user_timeline = user.timeline()\n",
        "df3 = extract_timeline_as_df(user_timeline)\n",
        "df3.head()"
      ],
      "metadata": {
        "id": "UrEuNil2eLpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4: Reply any tweet "
      ],
      "metadata": {
        "id": "-bpg4c7Yo3TX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# og_tweet = api.get_status(\"1584907651753746432\")\n",
        "# # print(og_tweet.user.screen_name, og_tweet.id)\n",
        "# my_reply = api.update_status(f\"@{og_tweet.user.screen_name} Wow this cool!\", og_tweet.id)\n",
        "#print(my_reply.id, my_reply.user.screen_name)"
      ],
      "metadata": {
        "id": "1ZaANxeqeLsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "74h8nKvueLvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5: Find all followers and friends count"
      ],
      "metadata": {
        "id": "qjKS8pJqs-Ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user = api.get_user(\"ipvikas\")\n",
        "print(user.followers_count, user.friends_count)"
      ],
      "metadata": {
        "id": "7FPndB4CeLx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_friends = user.friends()\n",
        "for friend in user_friends:\n",
        "  print(friend.screen_name)"
      ],
      "metadata": {
        "id": "TmCA2ZSeeL3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6: Items & Pagination with Tweepy Cursor"
      ],
      "metadata": {
        "id": "GcXjEea3thuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(api.home_timeline(count=20))"
      ],
      "metadata": {
        "id": "QjiQctOoeL7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "\n",
        "user = api.get_user(\"sudhirchaudhary\")\n",
        "\n",
        "for i, status in enumerate(tweepy.Cursor(api.home_timeline, count=50).items(50)):\n",
        "    print(i, status.text)"
      ],
      "metadata": {
        "id": "gTgOql-QeL-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d1C25pwHtk3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FifVzVjweMBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7: Search for any tweet from any start date"
      ],
      "metadata": {
        "id": "h7tspsUduXIl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PobBqzczqsXv"
      },
      "source": [
        "search_words = [\"#SuryaGrahan\"] # [\"#ipl2020\",\"#DCvKXIP\"]; [\"#coronavirus\",\"#Modi\"]; [\"#TwinTowers\"] \n",
        "date_since = \"2022-10-25\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K0KChiRqsbQ"
      },
      "source": [
        "tweets = tw.Cursor(api.search,\n",
        "              q=search_words,\n",
        "              lang=\"en\",\n",
        "              since=date_since).items(10)\n",
        "\n",
        "tweets\n",
        "\n",
        "#will take some time 2 min\n",
        "# tweet_details = [[tweet.geo, tweet.text,tweet.user.screen_name, tweet.user.location] for tweet in tweets]\n",
        "tweet_details = [[tweet.geo, tweet.text,tweet.user.screen_name, tweet.user.location,tweet.created_at,[e['text'] for e in tweet._json['entities']['hashtags']], tweet.user.followers_count] for tweet in tweets]\n",
        "tweet_details"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk_f3rAJqskZ"
      },
      "source": [
        "import pandas as pd\n",
        "tweet_df = pd.DataFrame(data=tweet_details, columns=['geo','text','user', \"location\",\"Timestamp\",\"All_hashtags\",\"Followers_Count\"])\n",
        "pd.set_option('max_colwidth', 80)\n",
        "tweet_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvuBzSntqsuQ"
      },
      "source": [
        "tweet_df.user.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir6yvwJhqsxR"
      },
      "source": [
        "tweet_df.location.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFTvCFatI7yN"
      },
      "source": [
        "\n",
        "# emoticons_happy = set([\n",
        "#     ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
        "#     ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
        "#     '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
        "#     'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
        "#     '<3'\n",
        "#     ])\n",
        " \n",
        "\n",
        "# emoticons_sad = set([\n",
        "#     ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
        "#     ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
        "#     ':c', ':{', '>:\\\\', ';('\n",
        "#     ])\n",
        " \n",
        "\n",
        "# emoticons = emoticons_happy.union(emoticons_sad)\n",
        "\n",
        "# import re\n",
        "# def clean_tweets(text):\n",
        "#     text = re.sub(\"RT @[\\w]*:\",\"\",text)\n",
        "#     text = re.sub(\"@[\\w]*\",\"\",text)\n",
        "#     text = re.sub(\"https?://[A-Za-z0-9./]*\",\"\",text)\n",
        "#     text = re.sub(\"\\n\",\"\",text)\n",
        "\n",
        "\n",
        "#     text = re.sub(\"\\$\\w*\",\"\",text)\n",
        "#     text = re.sub(\"^RT[\\s]+\",\"\",text)\n",
        "#     text = re.sub(\"https?:\\/\\/.*[\\r\\n]*\",\"\",text)\n",
        "#     text = re.sub(\"#\",\"\",text)\n",
        "\n",
        "\n",
        "#     tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "#     tweet_tokens = tokenizer.tokenize(text)\n",
        " \n",
        "#     tweets_clean = []    \n",
        "#     for word in tweet_tokens:\n",
        "#         if (word not in stopwords_english and\n",
        "#               word not in emoticons and \n",
        "#                 word not in string.punctuation): \n",
        "           \n",
        "#             stem_word = stemmer.stem(word)\n",
        "#             tweets_clean.append(stem_word)\n",
        " \n",
        "#     return tweets_clean\n",
        "#     text = tweets_clean \n",
        " \n",
        "\n",
        "\n",
        "\n",
        "#     # return text\n",
        "\n",
        "# tweet_df['text']=tweet_df['text'].apply(lambda x: clean_tweets(x))\n",
        "\n",
        "# tweet_df.head(20)\n",
        "# tweet_df.to_csv('tweets.csv')\n",
        "# # !ls\n",
        "\n",
        "\n",
        "\n",
        "# # tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "# # tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "# # tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "# # tweet = re.sub(r'#', '', tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqRgoUZSqs0k"
      },
      "source": [
        "import re\n",
        "def clean_tweets(text):\n",
        "    text = re.sub(\"RT @[\\w]*:\",\"\",text)\n",
        "    text = re.sub(\"@[\\w]*\",\"\",text)\n",
        "    text = re.sub(\"https?://[A-Za-z0-9./]*\",\"\",text)\n",
        "    text = re.sub(\"\\n\",\"\",text)\n",
        "\n",
        "\n",
        "    text = re.sub(\"\\$\\w*\",\"\",text)\n",
        "    text = re.sub(\"^RT[\\s]+\",\"\",text)\n",
        "    text = re.sub(\"https?:\\/\\/.*[\\r\\n]*\",\"\",text)\n",
        "    text = re.sub(\"#\",\"\",text)\n",
        "\n",
        "    return text\n",
        "\n",
        "tweet_df['text']=tweet_df['text'].apply(lambda x: clean_tweets(x))\n",
        "\n",
        "tweet_df.head(20)\n",
        "tweet_df.to_csv('tweets.csv')\n",
        "# !ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSnzO1STsNXB"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC43Ja1HsNaO"
      },
      "source": [
        "tweet_df['text'].apply(lambda x: [print(\"\\tText : {}, Entity : {}\".format(ent.text, ent.label_)) if (not ent.text.startswith('#')) else \"\"  for ent in nlp(x).ents])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XojGNFm0sNdO"
      },
      "source": [
        "tweet_df['entities']=tweet_df['text'].apply(lambda x: [(ent.text, ent.label_) if (not ent.text.startswith('#')) else \"\" for ent in nlp(x).ents])\n",
        "tweet_df.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_Kr2ISjsNjT"
      },
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "tweet_df['sentiment']=tweet_df['text'].apply(lambda x: sid.polarity_scores(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-0g3ztg9lyl"
      },
      "source": [
        "tweet_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7Qcy3w-sNwB"
      },
      "source": [
        "!pip install googlemaps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpMI4cZj42IF"
      },
      "source": [
        "import googlemaps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_gRoeBAsN_5"
      },
      "source": [
        "def get_country(input):\n",
        "  try:\n",
        "    output=gmaps.geocode(input)[0]['formatted_address'].split(\",\")[-1].strip()\n",
        "  except:\n",
        "    output=\"Error\"\n",
        "  return output\n",
        "\n",
        "tweet_df['country']=tweet_df['location'].apply(lambda x: \"\" if (not x.strip()) else get_country(x))\n",
        "\n",
        "tweet_df['country'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa7A8i7IGihn"
      },
      "source": [
        "tweet_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4CM-_l8GikY"
      },
      "source": [
        "tweet_df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNyu2DVtGipW"
      },
      "source": [
        "tweet_df.to_csv(\"tweets.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IW13LdeYgSHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ammtzUByGi09"
      },
      "source": [
        "#BASIC 1\n",
        "import csv\n",
        "import random\n",
        "from nltk.corpus import twitter_samples as ts\n",
        "import tweepy\n",
        "#in anaconda prompt: pip install tweepy\n",
        "from textblob import TextBlob\n",
        "\n",
        "consumer_key= 'du9vEcqQFCtidu9vEcqQFCti'\n",
        "consumer_secret= 'w7JsH0E4KCfOTMdcw7JsH0E4KCfOTMdc'\n",
        "\n",
        "access_token='126864770-A126864770-A'\n",
        "access_token_secret='JS1l5hXn9JS1l5hXn9'\n",
        "\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "#Step 3 - Retrieve Tweets\n",
        "public_tweets = api.search('SuryaGrahan')\n",
        "\n",
        "for tweet in public_tweets:\n",
        "    print(tweet.text)\n",
        "    \n",
        "    #Step 4 Perform Sentiment Analysis on Tweets\n",
        "    analysis = TextBlob(tweet.text)\n",
        "    print(analysis.sentiment)\n",
        "    print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx8V60p7x4TO"
      },
      "source": [
        "#BASIC 2: we have used it in above code\n",
        "# https://github.com/ritvikmath/ScrapingData\n",
        "import json\n",
        "import csv\n",
        "import tweepy\n",
        "#in anaconda prompt: pip install tweepy\n",
        "\n",
        "import re\n",
        "\n",
        "\"\"\"\n",
        "INPUTS:\n",
        "    consumer_key, consumer_secret, access_token, access_token_secret: codes \n",
        "    telling twitter that we are authorized to access this data\n",
        "    hashtag_phrase: the combination of hashtags to search for\n",
        "OUTPUTS:\n",
        "    none, simply save the tweet info to a spreadsheet\n",
        "\"\"\"\n",
        "\n",
        "def search_for_hashtags(consumer_key, consumer_secret, access_token, access_token_secret, hashtag_phrase):\n",
        "    #create authentication for accessing Twitter\n",
        "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "    auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "    #initialize Tweepy API\n",
        "    api = tweepy.API(auth)\n",
        "    \n",
        "    #get the name of the spreadsheet we will write to\n",
        "    fname = '_'.join(re.findall(r\"#(\\w+)\", hashtag_phrase))\n",
        "\n",
        "    #open the spreadsheet we will write to\n",
        "    with open('%s 2_tweets.csv' % (fname), 'w') as file:\n",
        "        w = csv.writer(file)\n",
        "\n",
        "        #write header row to spreadsheet\n",
        "        w.writerow(['timestamp', 'tweet_text', 'username', 'all_hashtags', 'followers_count'])\n",
        "\n",
        "        #for each tweet matching our hashtags, write relevant info to the spreadsheet\n",
        "        for tweet in tweepy.Cursor(api.search, q=hashtag_phrase+' -filter:retweets', lang=\"en\", tweet_mode='extended').items(100):\n",
        "            w.writerow([tweet.created_at, tweet.full_text.replace('\\n',' ').encode('utf-8'), tweet.user.screen_name.encode('utf-8'), [e['text'] for e in tweet._json['entities']['hashtags']], tweet.user.followers_count])\n",
        "            \n",
        "# consumer_key = input('Consumer Key')\n",
        "# consumer_secret = input('Consumer Secret')\n",
        "# access_token = input('Access Token')\n",
        "# access_token_secret = input('Access Token Secret')\n",
        "consumer_key= 'du9vEcqQFCtiiNSJYpDhjJwEW'\n",
        "consumer_secret= 'w7JsH0E4KCfOTMdcEtSKa3RSo8CImQVlDW6cSeujpBT2mKTigS'\n",
        "access_token='126864770-A5bLHXKcDFFkLqxFG296m56xlvGzskw4WMTIfRq5'\n",
        "access_token_secret='JS1l5hXn9dSM8lf2TCsX6g8JpLzFA5DyQ2znHGvgNr1rD'\n",
        "\n",
        "hashtag_phrase = input('Hashtag Phrase')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    search_for_hashtags(consumer_key, consumer_secret, access_token, access_token_secret, hashtag_phrase)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2otqas6x4Wl"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nIXzu_Ix4Zn"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tG4F2oNCnXws"
      },
      "source": [
        "## 2. Scrape Twitter Data or Tweets using snscrape module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNtgWHI3x4cs"
      },
      "source": [
        "!pip install -q snscrape\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import date, timedelta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8zQN0iIx4i4"
      },
      "source": [
        "today = date.today()\n",
        "end_date = today\n",
        "\n",
        "search_term = 'SuryaGrahan'\n",
        "from_date = date.today() - timedelta(days=1)# '2022-08-28'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvdsmSFhnkYr"
      },
      "source": [
        "####Total Number of Tweets for Search Terms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66CNFhk6o1aw"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd '/content/drive/My Drive/Colab Notebooks/NLP'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aqma09rx4sU"
      },
      "source": [
        "import os\n",
        "# from_date = '2021-08-15'\n",
        "# search_term = '#IndiaIndependenceDay'\n",
        "# end_date = '2021-08-16'\n",
        "\n",
        "os.system(f\"snscrape --since {from_date} twitter-search '{search_term} until:{end_date}' > result-tweets.txt\") #will take more than 10 minutes\n",
        "if os.stat(\"result-tweets.txt\").st_size == 0:\n",
        "  counter = 0\n",
        "else:\n",
        "  df = pd.read_csv('result-tweets.txt', names=['link'])\n",
        "  counter = df.size\n",
        "\n",
        "print('Number Of Tweets : '+ str(counter))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUafeVaanrWy"
      },
      "source": [
        "#####Extracting Exact Tweeets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcW6q591nmZX"
      },
      "source": [
        "max_results = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSL0xngDnmcj"
      },
      "source": [
        "extracted_tweets = \"snscrape --format '{content!r}'\"+ f\" --max-results {max_results} --since {from_date} twitter-search '{search_term} until:{end_date}' > extracted-tweets.txt\"\n",
        "\n",
        "# extracted_tweets = (re.sub('@[a-z,_,A-Z,0-9,:]+ ', '', i) for i in extracted_tweets)\n",
        "\n",
        "os.system(extracted_tweets)\n",
        "if os.stat(\"extracted-tweets.txt\").st_size == 0:\n",
        "  print('No Tweets found')\n",
        "else:\n",
        "  df = pd.read_csv('extracted-tweets.txt', names=['content'])\n",
        "  for row in df['content'].iteritems():\n",
        "    print(row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eXfEYa0pOqCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyLJl8NGnm1d"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKEwMM2Mx4vh"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}